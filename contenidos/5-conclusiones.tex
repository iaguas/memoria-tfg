\documentclass[main]{subfiles}

\begin{document}

\chapter{Conclusiones y líneas de futuro}\label{cap:conclusiones}

En primer lugar, con respecto a las funciones de Dombi, podemos comentar que estas parecían funcionar bien con una imagen y que sería determinante el parámetro $w$, siendo diferente para cada imagen. Esto parecía indicar que sería necesario poder entrenar con un conjunto de imágenes un sistema experto para poder obtener siempre la mejor umbralización en función de unos parámetros dados.

Al utilizar la función de Dombi en conjunto con otras o con ella misma es cuando se concluye que esta función no es adecuada para poder hacer segmentación de imagen. No al menos con la construcción de conjuntos difusos que se ha presentado en este estudio. Esto se debe a que las funciones de Dombi no cumplen de forma absoluta que para la función de equivalencia $e$, $e(x,x)=1$. Entran a solucionar la paradoja que se produce con la propiedad $e(x,c(x))=0$ lo que hace que se convierta en inútil para todas las conclusiones.

Con respecto al método que se ha presentado que utiliza las funciones penalti (sección \ref{sec:exp2}), se debe decir que consume mucho más tiempo de procesamiento. Ahora bien, ciertamente los errores comparados con los resultados obtenidos por otros métodos hacen ver el método satisface de manera muy buena la solución del problema. Se tiene que tener en cuenta que en la versión que se ha implementado con funciones de Dombi, estas, en vez de empeorar el resultado le hacen abarcar una mayor amplitud de posibilidades algo que hace mejorar este con respecto al algoritmo original ({\em outlayers}). Si bien es cierto que se nota una cierta mejoría de la propuesta cuando en el histograma, que dispone de varios mínimos, son más similares entre sí.

Por otra parte, con relación a las funciones OWA para sustituir a la media aritmética se comprueba experimentalmente que no es una buena idea para obtener una buena segmentación. Quizá esto se deba a que al ordenar el OWA, si hay dos píxeles diferentes con igual frecuencia de $h(q)$ se elige el de tonalidad más alta. Por esta razón, si el rango dinámico de la imagen es muy pequeño, habrá una serie de píxeles con tonos cercanos a 255 que obtengan un gran beneficio de ello.

Esta situación cambia cuando se utilizan las funciones penalti para obtener el mejor umbral. Aun así, esta situación no es la mejor salida posible para esta intención. Se ha de reconocer que el algoritmo de maximización de la similitud para obtener el umbral óptimo, si se utiliza la media aritmética para crear los conjuntos $Q_t$ y el OWA sin multiplicar por el valor $h(q)$, se obtiene el mejor resultado para toda esta batería de experimentos.

Ninguno de los algoritmos presentados funciona mejor con imágenes ruidosas que los ya presentados por otros autores. Esto es lógico ya que, por ejemplo, en el caso de tener ruido impulsivo esto es irresoluble. Por ejemplo, si un cierto píxel que originalmente pertenecía a una región ahora ha cambiado de forma que la tonalidad le hace caer en la otra es imposible conocer esto. Ahora bien, por medio de filtros existen técnicas para conocer si un píxel es adecuado a su entorno (estas no son aplicadas por la segmentación). En este caso, se podría probar cómo afecta el aplicar un filtro a la imagen para luego segmentarla, lo que podría solucionar el problema.

%En cuanto al contraste, se ha comprobado que en ningún momento se obtienen mejores resultados que el algoritmo original.\REV{faltan pruebas...}

Por todo ello, acabado el estudio se concluye que tanto el método de funciones penalti analizado como la variación que se hace sobre el algoritmo 3 son una buena forma de de segmentar y que esto mejora los resultados que se obtenian con las versiones originales de ambos algoritmos. Por esta razón, una forma de mejorar estas, la versión que utiliza la función penalti, sería dirimir qué funciones de agregación son las que más influyen en la decisión con intención de quedarse solo con ellas. Esto provocaría una reducción del tiempo de cómputo.

Otra posible mejora, en ambos algoritmos, tal y como planteaba este trabajo, sería sustituir o añadir más funciones para calcular la pertenencia de los conjuntos difusos, además de las REF. Esto en un principio podría crear un programa más pesado computacionalmente hablando pero si se eligen de forma adecuada las funciones que más ayudan a la decisión. Parece instantáneo pensar algo similiar a lo propuesto anteriormente de buscar un subconjunto de funciones para llevar a cabo menos cálculos.

Se hace curioso comprobar que para una imagen con y sin ruido como la silla se obtengan los umbrales más similares a otros métodos con el mismo valor para $w$. Esto hace suponer, por tanto, que sea aquí quizá donde la función alcance el pleno de la propiedad $e(x,x)=1$. En cualquier caso, como ya se ha dicho, esto es inútil ya que cada imagen necesita un valor de $w$ diferente por lo que no es posible llegar a un algoritmo universal salvo, quizá, a través de un sistema experto y algoritmos bioinspirados.
%\REV{Revisar la coherencia de esto con lo anterior}
%\REV{   Javier: Esto que digo aquí viene motivado pq en el artículo de Dombi insinua que soluciona la paradoja intentando que haya un parámetro que la regule, de forma que aunque las propiedades no son plenas, existan ambas al mismo tiempo. Creo que deberíamos hablarlo un min, ya que igual cambia las conclusiones.}

%Para dombi:
%\begin{itemize}
 %   \item El ruido es igual.
  %  \item Mismo $w$ con y sin ruido.
   % \item Penalti consume mucho más tiempo.
    %\item Problemas con las funciones iniciales.
%    \item La solución no funciona
%\end{itemize}

%Para OWA:
%\begin{itemize}
 %   \item Las OWA para los conjuntos difusos que representan imágenes no son buenos.
  %  \item La OWA sin frecuencia obtiene buenos resultados para la creación del conjunto que se utiliza en el 3 para la creación de la SM.
   % \item ¿Problemas en la creación de los OWA cuando se tiene que $h(q)=h(q')$?
%\end{itemize}

\end{document}
